{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sqlite3\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current date: 2023-03-02\n",
      "current url: https://towardsdatascience.com/archive\\2023\\03\n",
      "current date: 2023-01-31\n",
      "current url: https://towardsdatascience.com/archive\\2023\\01\n",
      "current date: 2023-01-01\n",
      "current url: https://towardsdatascience.com/archive\\2023\\01\n",
      "current date: 2022-12-02\n",
      "current url: https://towardsdatascience.com/archive\\2022\\12\n"
     ]
    }
   ],
   "source": [
    "# set the logging level\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "class PublicationScraper:\n",
    "    \"\"\"\n",
    "    A simple scraper that connects to the publication's archive\n",
    "    and collect all the articles published this month\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, publication: str, rollback=3, **kwargs):\n",
    "        self.publication_url = publication\n",
    "        self.archive_url = os.path.join(self.publication_url, \"archive\")\n",
    "        self.current_date = datetime.date.today()\n",
    "\n",
    "        self.target_urls = []\n",
    "        self.target_dates = []\n",
    "        self.pages = []\n",
    "\n",
    "        self.data = {}\n",
    "\n",
    "        self.check_archive()\n",
    "        if not self.archive_available:\n",
    "            raise NotImplementedError(\"Scraping from the profile page (without archive) is not supported.\")\n",
    "\n",
    "        if rollback:\n",
    "            for m in range(1, rollback + 2):\n",
    "                date = self.current_date.replace(day=1) - timedelta(1 * 30 * m)\n",
    "                #\n",
    "                month = str(date.month)\n",
    "                month = f\"0{month}\" if len(month) == 1 else str(month)\n",
    "                #\n",
    "                target_url = os.path.join(self.archive_url, str(date.year), month)\n",
    "\n",
    "                self.target_dates.append(date)\n",
    "                self.target_urls.append(target_url)\n",
    "\n",
    "        for date, url in zip(self.target_dates, self.target_urls):\n",
    "            logging.info(f\"Constructed the target url: {url}\")\n",
    "            req = requests.get(url)\n",
    "            logging.info(f\"Sent the GET request, status code: {req.status_code}\")\n",
    "\n",
    "            if req.url == os.path.join(self.archive_url, str(date.year)):\n",
    "                raise NotImplementedError(\n",
    "                    \"Scraping just from the year is not implemented yet. (URL: %s)\" % (self.publication_url)\n",
    "                )\n",
    "\n",
    "            page = BeautifulSoup(req.content, \"html.parser\")\n",
    "            logging.info(\"Initialized the beautifulsoup\")\n",
    "            self.pages.append(page)\n",
    "\n",
    "    def check_archive(self):\n",
    "        r = requests.get(self.archive_url)\n",
    "        if \"PAGE NOT FOUND\" in r.text and \"404\" in r.text:\n",
    "            self.archive_available = False\n",
    "        else:\n",
    "            self.archive_available = True\n",
    "\n",
    "    def scrape(self):\n",
    "        for page, date, url in zip(self.pages, self.target_dates, self.target_urls):\n",
    "            print(\"current date:\", date)\n",
    "            print(\"current url:\", url)\n",
    "            posts = self.get_posts(page)\n",
    "            data = self.get_data(posts, url)\n",
    "            self.data[str(date)] = data\n",
    "        return self.data\n",
    "\n",
    "    def scrape_profile(self):\n",
    "        \"\"\"\n",
    "        It should be noted that the Medium sends a POST request to the -author_medium_url-/_/batch with some authentication/bot analysis\n",
    "        headers. So it's highly possible to reverse-engineer the API and imitate a real client to collect the data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Not implemented yet.\")\n",
    "\n",
    "    def get_posts(self, page) -> list[str]:\n",
    "        posts = page.find_all(\"div\", {\"streamItem--postPreview\"})\n",
    "        return posts\n",
    "\n",
    "    def get_data(self, posts, url) -> list[dict]:\n",
    "        samples = []\n",
    "        for idx, post in enumerate(posts):\n",
    "            try:\n",
    "                author = post.find(\"div\", {\"class\": \"postMetaInline-authorLockup\"}).find(\"a\").text\n",
    "                uicaption = post.find(\"div\", {\"class\": \"ui-caption\"})\n",
    "                date = uicaption.find(\"a\").find(\"time\")[\"datetime\"]\n",
    "                reading_time = uicaption.find(\"span\", {\"class\": \"readingTime\"})[\"title\"]\n",
    "\n",
    "                article_content = post.find(\"div\", {\"class\": \"postArticle-content\"})\n",
    "                title = post.select(\"h3.graf--title\")[0].text\n",
    "                post_url = article_content.parent[\"href\"]\n",
    "                preview_image_url = article_content.find(\"figure\").find(\"img\")[\"src\"]\n",
    "                claps = post.find(\"div\", {\"class\": \"multirecommend\"}).find_all(\"span\")[-1].text\n",
    "\n",
    "                sample = {\n",
    "                    \"author\": author,\n",
    "                    \"date\": date,\n",
    "                    \"reading_time\": reading_time,\n",
    "                    \"post_url\": post_url,\n",
    "                    \"title\": title,\n",
    "                    \"preview_image_url\": preview_image_url,\n",
    "                    \"claps\": claps,\n",
    "                }\n",
    "\n",
    "                sample = self.post_process(sample)\n",
    "\n",
    "                samples.append(sample)\n",
    "            except Exception as e:\n",
    "                random_text = str(time.time()).replace(\".\", \"\")\n",
    "                debugfile_name = f\"debug_{random_text}.html\"\n",
    "                with open(debugfile_name, \"w\") as f:\n",
    "                    f.write(post.prettify())\n",
    "                print(\"An error occurred for the URL: %s | post saved as %s\" % (url, debugfile_name))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def post_process(self, sample: dict):\n",
    "        sample[\"date\"] = sample[\"date\"].split(\"T\")[0]\n",
    "        sample[\"reading_time\"] = \" \".join(sample[\"reading_time\"].split()[:-1])\n",
    "        sample[\"post_url\"] = sample[\"post_url\"].split(\"?source=collection_archive\")[0]\n",
    "        return sample\n",
    "\n",
    "\n",
    "scraper = PublicationScraper(\"https://towardsdatascience.com/\", rollback=3)\n",
    "data = scraper.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MediumScraper:\n",
    "    def __init__(self, publications, suppress=False, **publication_scraper_kwargs):\n",
    "        self.publications = publications\n",
    "        self.archives = list(map(lambda x: os.path.join(x, \"archive\"), publications))\n",
    "        self.scrapers = []\n",
    "        self.data = {}\n",
    "        for publication in publications:\n",
    "            if suppress:\n",
    "                try:\n",
    "                    scraper = PublicationScraper(publication, **publication_scraper_kwargs)\n",
    "                except Exception as e:\n",
    "                    print(\"An error occurred:\", e)\n",
    "                    print(\"Removing the publication: %s\" % (publication))\n",
    "                    self.publications.remove(publication)\n",
    "            else:\n",
    "                scraper = PublicationScraper(publication, **publication_scraper_kwargs)\n",
    "\n",
    "            self.scrapers.append((publication, scraper))\n",
    "\n",
    "        self.data = {p: [] for p, s in self.scrapers}\n",
    "\n",
    "    def scrape(self):\n",
    "        for pub, scraper in self.scrapers:\n",
    "            data = scraper.scrape()\n",
    "            self.data[pub] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications = [\n",
    "    \"https://towardsdatascience.com/\",\n",
    "    \"https://medium.com/swlh\",\n",
    "    \"https://humanparts.medium.com/\",\n",
    "    \"https://medium.com/geekculture\",\n",
    "    \"https://levelup.gitconnected.com/\",\n",
    "    \"https://python.plainenglish.io/\",\n",
    "    \"https://entrepreneurshandbook.co/\",\n",
    "]\n",
    "crawler = MediumScraper(publications, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "table articles already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m con \u001b[39m=\u001b[39m sqlite3\u001b[39m.\u001b[39mconnect(\u001b[39m\"\u001b[39m\u001b[39mmedium.db\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m cur \u001b[39m=\u001b[39m con\u001b[39m.\u001b[39mcursor()\n\u001b[1;32m----> 4\u001b[0m cur\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m      5\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mcreate table articles(publication_url, author, date, reading_time, post_url, title, preview_image_url, claps)\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[39m# sanity check\u001b[39;00m\n\u001b[0;32m      9\u001b[0m r \u001b[39m=\u001b[39m cur\u001b[39m.\u001b[39mexecute(\u001b[39m\"\u001b[39m\u001b[39mselect name from sqlite_master\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mOperationalError\u001b[0m: table articles already exists"
     ]
    }
   ],
   "source": [
    "con = sqlite3.connect(\"medium.db\")\n",
    "cur = con.cursor()\n",
    "\n",
    "cur.execute(\n",
    "    \"create table articles(publication_url, author, date, reading_time, post_url, title, preview_image_url, claps)\"\n",
    ")\n",
    "\n",
    "# sanity check\n",
    "r = cur.execute(\"select name from sqlite_master\")\n",
    "r.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: https://towardsdatascience.com/\n",
      "Date: 2022-12-02\t 9\n",
      "Date: 2022-11-02\t 8\n",
      "Date: 2022-10-03\t 10\n",
      "Date: 2022-09-03\t 8\n",
      "Key: https://medium.com/swlh\n",
      "Date: 2022-12-02\t 10\n",
      "Date: 2022-11-02\t 10\n",
      "Date: 2022-10-03\t 10\n",
      "Date: 2022-09-03\t 10\n",
      "Key: https://humanparts.medium.com/\n",
      "Date: 2022-12-02\t 3\n",
      "Date: 2022-11-02\t 4\n",
      "Date: 2022-10-03\t 6\n",
      "Date: 2022-09-03\t 3\n",
      "Key: https://medium.com/geekculture\n",
      "Date: 2022-12-02\t 10\n",
      "Date: 2022-11-02\t 10\n",
      "Date: 2022-10-03\t 10\n",
      "Date: 2022-09-03\t 10\n",
      "Key: https://levelup.gitconnected.com/\n",
      "Date: 2022-12-02\t 10\n",
      "Date: 2022-11-02\t 10\n",
      "Date: 2022-10-03\t 10\n",
      "Date: 2022-09-03\t 10\n",
      "Key: https://python.plainenglish.io/\n",
      "Date: 2022-12-02\t 10\n",
      "Date: 2022-11-02\t 10\n",
      "Date: 2022-10-03\t 10\n",
      "Date: 2022-09-03\t 10\n",
      "Key: https://entrepreneurshandbook.co/\n",
      "Date: 2022-12-02\t 10\n",
      "Date: 2022-11-02\t 10\n",
      "Date: 2022-10-03\t 10\n",
      "Date: 2022-09-03\t 10\n"
     ]
    }
   ],
   "source": [
    "for k, v in crawler.data.items():\n",
    "    print(\"Key: %s\" % k)\n",
    "    for vk, vv in v.items():\n",
    "        print(\"Date: %s\" % vk, end=\"\")\n",
    "        print(\"\\t %d\" % len(vv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m commit_data \u001b[39m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m crawler\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems():\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mfor\u001b[39;00m kk, vv \u001b[39min\u001b[39;00m v\u001b[39m.\u001b[39;49mitems():\n\u001b[0;32m      5\u001b[0m         \u001b[39mfor\u001b[39;00m post \u001b[39min\u001b[39;00m vv:\n\u001b[0;32m      6\u001b[0m             entry \u001b[39m=\u001b[39m [k] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m], post\u001b[39m.\u001b[39mitems()))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "commit_data = []\n",
    "\n",
    "for k, v in crawler.data.items():\n",
    "    for kk, vv in v.items():\n",
    "        for post in vv:\n",
    "            entry = [k] + list(map(lambda x: x[1], post.items()))\n",
    "            commit_data.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in commit_data:\n",
    "    text_entry = \"'\" + \"','\".join(entry) + \"'\"\n",
    "    cur.execute(f\"INSERT INTO articles VALUES ({text_entry})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"SELECT * from articles\", con)\n",
    "df.to_csv(\"dataset.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
