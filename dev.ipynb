{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current date: 2022-12-02\n",
      "current url: https://towardsdatascience.com/archive/2022/12\n",
      "An error occurred for the URL: https://towardsdatascience.com/archive/2022/12 | post saved as debug_1675190872883691.html\n",
      "current date: 2022-11-02\n",
      "current url: https://towardsdatascience.com/archive/2022/11\n",
      "An error occurred for the URL: https://towardsdatascience.com/archive/2022/11 | post saved as debug_16751908728971431.html\n",
      "An error occurred for the URL: https://towardsdatascience.com/archive/2022/11 | post saved as debug_1675190872901088.html\n",
      "current date: 2022-10-03\n",
      "current url: https://towardsdatascience.com/archive/2022/10\n",
      "current date: 2022-09-03\n",
      "current url: https://towardsdatascience.com/archive/2022/09\n",
      "An error occurred for the URL: https://towardsdatascience.com/archive/2022/09 | post saved as debug_16751908729298382.html\n",
      "An error occurred for the URL: https://towardsdatascience.com/archive/2022/09 | post saved as debug_1675190872930725.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import logging\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# set the logging level\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "class PublicationScraper:\n",
    "    \"\"\"\n",
    "        A simple scraper that connects to the publication's archive\n",
    "        and collect all the articles published this month\n",
    "    \"\"\"\n",
    "    def __init__(self, publication: str, rollback=3, **kwargs):\n",
    "        self.publication_url = publication\n",
    "        self.archive_url = os.path.join(self.publication_url, \"archive\")\n",
    "        self.current_date = datetime.date.today()\n",
    "\n",
    "        self.target_urls = []\n",
    "        self.target_dates = []\n",
    "        self.pages = []\n",
    "\n",
    "        self.data = {}\n",
    "\n",
    "        self.check_archive()\n",
    "        if not self.archive_available:\n",
    "            raise NotImplementedError(\"Scraping from the profile page (without archive) is not supported.\")\n",
    "\n",
    "        if rollback:\n",
    "            for m in range(1, rollback+2):\n",
    "                date = self.current_date.replace(day=1) - timedelta(1*30*m)\n",
    "                #\n",
    "                month = str(date.month)\n",
    "                month = f\"0{month}\" if len(month)==1 else str(month)\n",
    "                #\n",
    "                target_url = os.path.join(self.archive_url, str(date.year), month)\n",
    "                \n",
    "                self.target_dates.append(date)\n",
    "                self.target_urls.append(target_url)\n",
    "\n",
    "        for date, url in zip(self.target_dates, self.target_urls):\n",
    "            logging.info(f\"Constructed the target url: {url}\")\n",
    "            req = requests.get(url)\n",
    "            logging.info(f\"Sent the GET request, status code: {req.status_code}\")\n",
    "\n",
    "            if req.url == os.path.join(self.archive_url, str(date.year)):\n",
    "                raise NotImplementedError(\"Scraping just from the year is not implemented yet. (URL: %s)\"%(self.publication_url))\n",
    "\n",
    "            page = BeautifulSoup(req.content, 'html.parser')\n",
    "            logging.info(\"Initialized the beautifulsoup\")\n",
    "            self.pages.append(page)\n",
    "\n",
    "    def check_archive(self):\n",
    "        r = requests.get(self.archive_url)\n",
    "        if \"PAGE NOT FOUND\" in r.text and \"404\" in r.text:\n",
    "            self.archive_available = False\n",
    "        else:\n",
    "            self.archive_available = True\n",
    "\n",
    "    def scrape(self):\n",
    "        for page, date, url in zip(self.pages, self.target_dates, self.target_urls):\n",
    "            print(\"current date:\", date)\n",
    "            print(\"current url:\", url)\n",
    "            posts = self.get_posts(page)\n",
    "            data = self.get_data(posts, url)\n",
    "            self.data[str(date)] = data\n",
    "        return self.data\n",
    "\n",
    "    def scrape_profile(self):\n",
    "        \"\"\"\n",
    "        It should be noted that the Medium sends a POST request to the -author_medium_url-/_/batch with some authentication/bot analysis \n",
    "        headers. So it's highly possible to reverse-engineer the API and imitate a real client to collect the data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Not implemented yet.\")\n",
    "\n",
    "    def get_posts(self, page) -> list[str]:\n",
    "        posts = page.find_all(\"div\", {\"streamItem--postPreview\"})\n",
    "        return posts\n",
    "    \n",
    "    def get_data(self, posts, url) -> list[dict]:\n",
    "        samples = []\n",
    "        for idx, post in enumerate(posts):\n",
    "            try:\n",
    "                author = post.find(\"div\", {\"class\":\"postMetaInline-authorLockup\"}).find(\"a\").text\n",
    "                uicaption = post.find(\"div\", {\"class\": \"ui-caption\"})\n",
    "                date = uicaption.find(\"a\").find(\"time\")['datetime']\n",
    "                reading_time = uicaption.find(\"span\", {\"class\": \"readingTime\"})['title']\n",
    "                \n",
    "\n",
    "                article_content = post.find(\"div\", {\"class\": \"postArticle-content\"})\n",
    "                title = post.select(\"h3.graf--title\")[0].text\n",
    "                post_url = article_content.parent['href']\n",
    "                preview_image_url = article_content.find(\"figure\").find(\"img\")['src']\n",
    "                claps = post.find(\"div\", {\"class\": \"multirecommend\"}).find_all(\"span\")[-1].text\n",
    "\n",
    "                sample = {\n",
    "                    \"author\": author,\n",
    "                    \"date\": date,\n",
    "                    \"reading_time\": reading_time,\n",
    "                    \"post_url\": post_url,\n",
    "                    \"title\": title,\n",
    "                    \"preview_image_url\": preview_image_url,\n",
    "                    \"claps\": claps\n",
    "                }\n",
    "\n",
    "                sample = self.post_process(sample)\n",
    "\n",
    "                samples.append(sample)\n",
    "            except Exception as e:\n",
    "                random_text = str(time.time()).replace(\".\", \"\")\n",
    "                debugfile_name = f\"debug_{random_text}.html\"\n",
    "                with open(debugfile_name, \"w\") as f:\n",
    "                    f.write(post.prettify())\n",
    "                print(\"An error occurred for the URL: %s | post saved as %s\"%(url, debugfile_name))\n",
    "                \n",
    "\n",
    "        return samples\n",
    "\n",
    "    def post_process(self, sample: dict):\n",
    "        sample['date'] = sample['date'].split(\"T\")[0]\n",
    "        sample['reading_time'] = \" \".join(sample[\"reading_time\"].split()[:-1])\n",
    "        sample['post_url'] = sample['post_url'].split(\"?source=collection_archive\")[0]\n",
    "        return sample\n",
    "\n",
    "scraper = PublicationScraper(\"https://towardsdatascience.com/\", rollback=3)\n",
    "data = scraper.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MediumScraper:\n",
    "    def __init__(self, publications, suppress=False, **publication_scraper_kwargs):\n",
    "        self.publications = publications\n",
    "        self.archives = list(map(lambda x: os.path.join(x, \"archive\"), publications))\n",
    "        self.scrapers = []\n",
    "        self.data = {}\n",
    "        for publication in publications:\n",
    "            if suppress:\n",
    "                try:\n",
    "                    scraper = PublicationScraper(publication, **publication_scraper_kwargs)\n",
    "                except Exception as e:\n",
    "                    print(\"An error occurred:\", e)\n",
    "                    print(\"Removing the publication: %s\"%(publication))\n",
    "                    self.publications.remove(publication)\n",
    "            else:\n",
    "                scraper = PublicationScraper(publication, **publication_scraper_kwargs)\n",
    "\n",
    "            self.scrapers.append((publication, scraper))\n",
    "        \n",
    "        self.data = {p: [] for p,s in self.scrapers}\n",
    "        \n",
    "    def scrape(self):\n",
    "        for pub, scraper in self.scrapers:\n",
    "            data = scraper.scrape()\n",
    "            self.data[pub] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications = [\n",
    "    'https://towardsdatascience.com/',\n",
    "    'https://medium.com/swlh',\n",
    "    \"https://humanparts.medium.com/\",\n",
    "    \"https://medium.com/geekculture\",\n",
    "    \"https://levelup.gitconnected.com/\",\n",
    "    \"https://python.plainenglish.io/\",\n",
    "    \"https://entrepreneurshandbook.co/\"\n",
    "]\n",
    "crawler = MediumScraper(publications, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: https://towardsdatascience.com/\n",
      "Date: 2022-12-02\t 9\n",
      "Date: 2022-11-02\t 8\n",
      "Date: 2022-10-03\t 10\n",
      "Date: 2022-09-03\t 8\n",
      "Key: https://medium.com/swlh\n",
      "Date: 2022-12-02\t 10\n",
      "Date: 2022-11-02\t 10\n",
      "Date: 2022-10-03\t 10\n",
      "Date: 2022-09-03\t 10\n",
      "Key: https://humanparts.medium.com/\n",
      "Date: 2022-12-02\t 3\n",
      "Date: 2022-11-02\t 4\n",
      "Date: 2022-10-03\t 6\n",
      "Date: 2022-09-03\t 3\n",
      "Key: https://medium.com/geekculture\n",
      "Date: 2022-12-02\t 10\n",
      "Date: 2022-11-02\t 10\n",
      "Date: 2022-10-03\t 10\n",
      "Date: 2022-09-03\t 10\n",
      "Key: https://levelup.gitconnected.com/\n",
      "Date: 2022-12-02\t 10\n",
      "Date: 2022-11-02\t 10\n",
      "Date: 2022-10-03\t 10\n",
      "Date: 2022-09-03\t 10\n",
      "Key: https://python.plainenglish.io/\n",
      "Date: 2022-12-02\t 10\n",
      "Date: 2022-11-02\t 10\n",
      "Date: 2022-10-03\t 10\n",
      "Date: 2022-09-03\t 10\n",
      "Key: https://entrepreneurshandbook.co/\n",
      "Date: 2022-12-02\t 10\n",
      "Date: 2022-11-02\t 10\n",
      "Date: 2022-10-03\t 10\n",
      "Date: 2022-09-03\t 10\n"
     ]
    }
   ],
   "source": [
    "for k,v in crawler.data.items():\n",
    "    print(\"Key: %s\"%k)\n",
    "    for vk, vv in v.items():\n",
    "        print(\"Date: %s\"%vk, end=\"\")\n",
    "        print(\"\\t %d\"%len(vv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('articles',)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = sqlite3.connect(\"medium.db\")\n",
    "cur = con.cursor()\n",
    "\n",
    "cur.execute(\"create table articles(publication_url, author, date, reading_time, post_url, title, preview_image_url, claps)\")\n",
    "\n",
    "# sanity check\n",
    "r = cur.execute(\"select name from sqlite_master\")\n",
    "r.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_data = []\n",
    "\n",
    "for k,v in crawler.data.items():\n",
    "    for kk, vv in v.items():\n",
    "        for post in vv:\n",
    "            entry = [k] + list(map(lambda x: x[1], post.items()))\n",
    "            commit_data.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in commit_data:\n",
    "    text_entry = \"'\" + \"','\".join(entry) + \"'\"\n",
    "    cur.execute(f\"INSERT INTO articles VALUES ({text_entry})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"SELECT * from articles\", con)\n",
    "df.to_csv(\"dataset.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
